<html><body><pre style="word-wrap: break-word; white-space: pre-wrap;">@inproceedings{zhang2025notellm,
  author = {Zhang, Chao and Zhang, Haoxin and Wu, Shiwei and Wu, Di and Xu, Tong and Zhao, Xiangyu and Gao, Yan and Hu, Yao and Chen, Enhong},
  title = {NoteLLM-2: Multimodal Large Representation Models for Recommendation},
  year = {2025},
  isbn = {9798400712456},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3690624.3709440},
  doi = {10.1145/3690624.3709440},
  abstract = {Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at https://github.com/Applied-Machine-Learning-Lab/NoteLLM.},
  booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1},
  pages = {2815â€“2826},
  numpages = {12},
  keywords = {multimodal large language model, multimodal representation, recommendation},
  location = {Toronto ON, Canada},
  series = {KDD '25}
}
</pre></body></html>
